# Kubernetes Project: Comprehensive Step-by-Step Guide

[//]: # (TOC)
## Table of Contents
1. [Day 1: Kubernetes Fundamentals](#day-1-kubernetes-fundamentals)
   - 1.1 [Research Kubernetes](#11-research-kubernetes)
   - 1.2 [Cluster Setup](#12-cluster-setup)
   - 1.3 [Basic Deployments](#13-basic-deployments)
2. [Day 2: Persistent Storage & Scaling](#day-2-persistent-storage--scaling)
   - 2.1 [Persistent Volumes](#21-persistent-volumes)
   - 2.2 [Autoscaling Strategies](#22-autoscaling-strategies)
3. [Day 3: Cloud Deployment & Networking](#day-3-cloud-deployment--networking)
   - 3.1 [Minikube on Cloud VM](#31-minikube-on-cloud-vm)
   - 3.2 [Advanced Networking](#32-advanced-networking)
[//]: # (TOC_END)

![Kubernetes Architecture](https://mermaid.ink/svg/eyJjb2RlIjoiZ3JhcGggVERcbiAgQ29udHJvbFBsYW5lW0NvbnRyb2wgUGxhbmVdXG4gIEV0Y2RbRXRjZCBLVSBkYXRhc3RvcmVdXG4gIEFQSVNlcnZlcltBUEkgU2VydmVyXVxuICBDb250cm9sbGVyX0NvbnRyb2xsZXJdXG4gIFNjaGVkdWxlcltTY2hlZHVsZXJdXG4gIFdvcmtlck5vZGVzW1dvcmtlciBOb2Rlc11cbiAgRG9ja2VyX0VuZ2luZVtEb2NrZXIgRW5naW5lXVxuICBLdWJlbGV0W0t1YmVsZXRdXG4gIEt1YmVQcm94eVtLdWJlLVByb3h5XVxuICBDb250cm9sUGxhbmUgLS0-fEV0Y2RcbiAgQ29udHJvbFBsYW5lIC0tPnxBUElTZXJ2ZXJcbiAgQ29udHJvbFBsYW5lIC0tPnxDb250cm9sbGVyX0NvbnRyb2xsZXJcbiAgQ29udHJvbFBsYW5lIC0tPnxTY2hlZHVsZXJcbiAgV29ya2VyTm9kZXMgLS0-fEt1YmVsZXRcbiAgV29ya2VyTm9kZXMgLS0-fERvY2tlcl9FbmdpbmVcbiAgV29ya2VyTm9kZXMgLS0-fEt1YmVQcm94eVxuIiwibWVybWFpZCI6eyJ0aGVtZSI6ImRlZmF1bHQifSwidXBkYXRlRWRpdG9yIjpmYWxzZX0)

This guide provides a structured walkthrough of Kubernetes fundamentals and advanced features across three days. Each section includes:
- Detailed technical explanations
- Production-grade YAML manifests
- Hands-on exercises with troubleshooting
- Architectural diagrams
- Security best practices
Day 1: Kubernetes Fundamentals and Basic Deployments

1. Research Kubernetes
Why is Kubernetes needed?

Containers revolutionized software deployment by packaging applications with their dependencies, but running containers at scale introduced new challenges. As organizations began deploying hundreds or thousands of containers, they needed a system to automate scheduling, scaling, networking, and fault-tolerance. Simply using containers (e.g. with Docker alone) lacked key capabilities such as monitoring container health, re-scheduling failed containers, scaling out/in based on load, and service discovery​
DEVTRON.AI
​
DEVTRON.AI
. Kubernetes was created to address these gaps. It is an open-source container orchestration platform that automates the management of containerized applications across clusters of machines. In essence, Kubernetes is needed to ensure that your application containers are highly available, can scale on demand, and are resilient to failures without manual intervention​
DEVTRON.AI
​
DEVTRON.AI
.
Benefits of Kubernetes

Kubernetes has become the de facto standard for container orchestration due to its numerous benefits. Key advantages include automated scalability, self-healing, and efficient resource utilization. Kubernetes can automatically scale applications up or down based on metrics like CPU/memory usage (Horizontal Pod Autoscaling), ensuring your app meets demand without over-provisioning​
DEVTRON.AI
. It provides high availability by detecting failed containers/pods and rescheduling replacements automatically​
DEVTRON.AI
. Kubernetes is cloud-agnostic and portable – it runs on public clouds (AWS, Azure, GCP), on-premises, or hybrid environments, preventing lock-in and allowing consistent deployment across environments​
DEVTRON.AI
. It optimizes infrastructure costs through bin-packing, achieving high resource efficiency by tightly scheduling containers on nodes​
DEVTRON.AI
. Moreover, Kubernetes boasts a huge ecosystem and community support, with many open-source tools and extensions (service meshes, operators, CI/CD integrations) available​
DEVTRON.AI
. All these benefits make it easier to deploy and manage complex, distributed applications in production.
Success stories of Kubernetes adoption

Many organizations have successfully adopted Kubernetes to achieve faster development and deployment cycles. For example, Spotify migrated from their homegrown container platform to Kubernetes, and found that teams spent less time on manual capacity provisioning and more time building features. One of Spotify’s services handles ~10 million requests per second and benefits greatly from Kubernetes’ autoscaling​
KUBERNETES.IO
. They also reduced service deployment time from hours to seconds, and improved CPU utilization 2–3x through Kubernetes’ efficient scheduling​
KUBERNETES.IO
. Another success story is Chick-fil-A (a major restaurant chain) using Kubernetes at the edge in each restaurant – this enabled rapid iteration and reliable operations even outside traditional data centers​
APPVIA.IO
. The Babylon Health case study highlights Kubernetes as a great platform for machine learning workloads due to its built-in scheduling and scalability​
KUBERNETES.IO
. These real-world cases demonstrate how Kubernetes can increase developer velocity, reduce infrastructure costs, and handle massive scale. Companies often note that Kubernetes’ strong open-source community and standard APIs let them innovate faster and adopt best practices used industry-wide​
KUBERNETES.IO
​
KUBERNETES.IO
.
Kubernetes architecture: clusters, nodes, and components



Kubernetes architecture consists of a control plane (master node components) and worker nodes (where containers run). This diagram shows a high-level view: the control plane components (API server, scheduler, controller manager, etcd) manage the cluster, while each worker node runs a kubelet agent, kube-proxy, and container runtime to host Pods.A cluster is the basic execution environment for Kubernetes – it’s a set of machines (virtual or physical) that work together to run your containerized applications. A cluster has two types of nodes:
Control Plane node(s) (sometimes called master node(s)): These run the core Kubernetes control plane components that manage the overall cluster state and orchestrate the workload. Key components include:
API Server – the front-end that exposes the Kubernetes API (receives kubectl commands and cluster interactions).
Controller Manager – runs controllers that enforce the desired state (e.g. ensuring the correct number of pod replicas).
Scheduler – schedules pods to run on specific nodes based on resource availability and other constraints.
etcd – a distributed key-value store that holds the cluster state and configuration.
Worker Nodes: These nodes actually run the application Pods. Each worker node has:
a kubelet – the node agent that communicates with the control plane and manages pods on that node.
a container runtime – such as Docker or containerd, to run the containers.
a kube-proxy – handles networking, routing traffic to the correct pod instances across the cluster.
Every cluster requires at least one worker node to run pods​
KUBERNETES.IO
. The control plane can be single or multiple nodes for redundancy; in production it is usually running on multiple nodes for high availability​
KUBERNETES.IO
. The control plane and worker nodes together form the full cluster. The control plane makes decisions about scheduling and scaling, while worker nodes execute the decisions (running or stopping containers). This separation is often referred to as the control plane vs. data plane distinction: the control plane (brain of the cluster) manages the data plane (the workhorses that run app data and workloads).In a managed Kubernetes service (like GKE, EKS, AKS), the cloud provider runs the control plane components for you (abstracting away master node management), whereas in a self-hosted cluster (e.g., using kubeadm or kops on your own servers) you are responsible for setting up and maintaining both control plane and worker nodes. The architecture is conceptually the same, but managed services offload operational burden (upgrades, HA, backups of etcd) at the cost of some flexibility. Self-hosting gives full control and customization (you can access all components), but requires more expertise to manage and can be more error-prone if not maintained properly​
GCORE.COM
​
GCORE.COM
. In summary, a managed Kubernetes cluster simplifies deployment and reduces ops overhead (the provider handles the masters and often provides SLA-backed uptime)​
GCORE.COM
, whereas a self-managed cluster gives you freedom to tailor every component but incurs higher operational complexity​
GCORE.COM
​
GCORE.COM
. Many teams choose managed services for production to leverage cloud provider reliability and focus on their apps, unless they have specific needs that require self-management.
Kubernetes objects: Pods, ReplicaSets, Deployments (and why Pods are ephemeral)

In Kubernetes, the fundamental unit of deployment is the Pod. A Pod represents one or more tightly-coupled containers (such as an app container and a sidecar) that share the same network IP and storage volumes. In practice, most Pods contain a single main container (e.g., your application container). Pods are intended to be ephemeral – they can be created and destroyed frequently by the system​
KUBERNETES.IO
​
KUBERNETES.IO
. You should not treat any individual Pod as a long-lived, pet-like server. If a Pod dies (due to node failure or other issue), Kubernetes will create a new one to replace it if managed by a higher-level controller. This ephemeral nature is by design: it enables self-healing and auto-scaling. However, it also means that any data stored inside a Pod’s container filesystem will be lost when the Pod is deleted or crashes. You mitigate this by using external storage (persistent volumes) and by using controllers to automatically recreate pods.ReplicaSet and Deployment are Kubernetes objects that manage Pods and provide resilience:
A ReplicaSet ensures a specified number of Pod replicas are running at any given time. If a Pod fails or is deleted, the ReplicaSet controller will launch a new Pod to meet the replica count. ReplicaSets monitor the cluster state and replace pods that crash or are terminated, which ensures high availability​
SEMATEXT.COM
​
SEMATEXT.COM
.
A Deployment is an even higher-level object that manages ReplicaSets (and thus Pods). Deployments provide declarative updates to Pods and ReplicaSets – for example, you can update the Pod template (e.g., to a new container image version) and the Deployment will roll out the change gradually (rolling update) while maintaining the desired number of Pods. Deployments make it easier to perform versioned upgrades, rollbacks, and scaling. When you create a Deployment, it automatically creates a ReplicaSet which in turn creates the Pods. You almost always use Deployments (rather than directly managing ReplicaSets) for stateless applications.
Why are Pods ephemeral? Kubernetes is built with the paradigm that individual pods come and go, and the system (via controllers) will continuously drive toward the desired state. Pods are not durable because treating them as replaceable units enables powerful orchestration capabilities like rescheduling on different nodes, auto-recovery, and scaling. The trade-off is you should design your app to not rely on any given Pod’s local state. Instead, externalize state (e.g., databases or persistent volumes) and rely on stable network endpoints (Services) rather than Pod IPs. To handle pod ephemerality:
Use a Deployment or other controller so that if a Pod dies, a new one is created automatically. For example, if you manually delete a Pod that was created by a Deployment, the Deployment’s ReplicaSet will notice the pod count dropped and will spawn a replacement within seconds.
Use PersistentVolumes (PV) for data that needs to persist across Pod restarts (we’ll cover PV/PVC in Day 2). A Pod can be attached to a persistent volume, so even if the Pod is destroyed, the data remains and can be reattached to a new Pod.
Use a Service to abstract Pod endpoints. Since pods get recreated (with new IPs), clients shouldn’t talk directly to a Pod’s IP. Instead, they talk to a Service (with a stable IP or DNS name), which routes to whatever Pods are currently backing that service.
In short, pods are ephemeral and not meant to be individually managed or relied upon for long durations​
KUBERNETES.IO
​
KUBERNETES.IO
. Higher-level controllers (Deployments, StatefulSets, etc.) and Kubernetes mechanisms ensure that the collective service provided by pods is persistent even if individual pod instances are not.
Security considerations for Kubernetes

Containerized applications bring security benefits (isolated environments, immutability) but also new challenges. When deploying on Kubernetes, you must consider security at multiple layers: the container image, the cluster configuration, network policies, and runtime. Here are some container and Kubernetes security best practices:
Use trusted, up-to-date images: Ensure your container images are from a trusted source or registry, and are regularly updated to include the latest security patches. “Well-maintained images” (e.g., official images or those maintained by reputable vendors) are preferred because they are kept at consistent patch levels, reducing known vulnerabilities​
ENTERPRISERSPROJECT.COM
. Avoid running images with known CVEs or pulling from unknown sources. Always scan images for vulnerabilities and rebuild them when base images have updates.
Least privilege: Follow the principle of least privilege both for containers and for Kubernetes API access. For containers, this means avoid running as root user inside the container if not necessary, and restrict capabilities (using Pod Security Context and SecurityContext settings). For the cluster, use Kubernetes RBAC to grant minimal permissions to users, service accounts, and CI/CD systems. Define Roles and ClusterRoles with only the needed access and bind them appropriately​
ACCUKNOX.COM
. This prevents an exploit in one component from easily compromising the whole cluster.
Network policies: Use Kubernetes NetworkPolicy to limit pod-to-pod communication where appropriate (for example, your frontend pods might only be allowed to talk to backend API pods and not to the database directly). This can contain the blast radius if one pod is compromised. By default, Kubernetes allows all cross-pod traffic; applying NetworkPolicies lets you whitelist expected traffic.
Protect the control plane: If you manage your own Kubernetes masters, ensure the API server is secure (use authentication and authorization, enable audit logging, etc.). For any cluster, secure etcd (encrypt secrets at rest, restrict access) since etcd holds all cluster state (including Secrets).
Regular updates: Keep your Kubernetes version and dependencies up to date. Kubernetes frequently releases patches for security issues. Similarly, keep the node OS and container runtime updated. Apply security fixes regularly to avoid known exploits.
Runtime security monitoring: Employ tools that can monitor running containers for suspicious behavior (e.g., Falco or other intrusion detection). Enable audit logs in Kubernetes to track changes to resources​
ACCUKNOX.COM
.
Pod security context & policies: Define pod security contexts to drop unneeded Linux capabilities, make file systems read-only where possible, and use seccomp/AppArmor profiles. Kubernetes is moving toward Pod Security Standards – you can enforce policies (baseline/restricted) to disallow privileged containers or dangerous host mounts cluster-wide.
Use Namespaces for multitenancy: Namespaces provide a scope for names and can help separate teams or environments. They can also be coupled with RBAC to ensure one team’s application cannot access another’s resources​
ACCUKNOX.COM
.
Enable logging and audit: Ensure that you capture logs from your applications and cluster. Kubernetes doesn’t automatically centralize application logs, so use a logging agent (EFK stack or cloud log service). Also use kubectl logs and events to troubleshoot issues. Kubernetes audit logging, when enabled, can record every call made to the API – useful for security forensics.
A key thing to remember is that container security extends to the supply chain as well. It’s not only about the cluster – you should also secure your CI/CD pipeline (to prevent image tampering) and use tools to sign and verify images (so that only trusted images run). By incorporating security checks early (DevSecOps approach), you can catch misconfigurations or vulnerabilities before deployment.
Maintained images: what are they and their pros/cons?

“Maintained images” refer to container images that are regularly updated and curated, typically by a vendor, open-source community, or your internal team. Examples include official images on Docker Hub (like nginx:latest or node:lts) or images your organization builds with a formal patching process. Using maintained base images is a best practice for security and stability.Pros: A well-maintained image is kept up-to-date with security patches and bug fixes. As one expert noted, “The best images are well-maintained and fairly immutable… layers are kept at consistent patch levels to protect against known vulnerabilities”​
ENTERPRISERSPROJECT.COM
. This means if a critical CVE in a library is announced, the maintainers will release a new image version addressing it, and you can update your deployments. Maintained images also often have a smaller attack surface (unneeded packages removed) and are tested by a broader community. For example, using the official Node.js or Python images ensures you get the latest minor updates and a base that the community trusts and monitors.Cons: On the flip side, relying on external maintained images means you need to track their updates and test your application with new versions. An update to a base image could potentially introduce changes that affect your app. There’s also a slight risk that an image could be deprecated or the maintainer could make breaking changes (for instance, switching the base OS from Debian to Alpine). Another con is that maintained images often prioritize stability, so they might not include the absolute latest version of a software if it’s experimental – which could be a limitation if you need a cutting-edge feature. Lastly, using a third-party maintained image requires trust in the maintainer; you should verify the source and integrity (many official images are signed or come from Docker Official Images or Verified Publishers, which mitigates this).In summary, the pros of maintained images far outweigh the cons for most cases. They give you a secure starting point and reduce the maintenance burden on your team. The best practice is to start with a minimal, well-maintained base image and only add what your app needs​
ACCUKNOX.COM
 (smaller images are not only more secure but also faster to pull and deploy). If you do customize an image, your team becomes the “maintainer” for that image – then you must regularly rebuild it with patches (which is essentially what a maintained image would do for you). Many organizations use renovate bots or CI pipelines to automatically rebuild images when base image updates are available, ensuring their derived images remain maintained.
2. Get Kubernetes running using Docker Desktop
One of the simplest ways to set up a local single-node Kubernetes cluster is by using Docker Desktop. Docker Desktop includes an option to enable a Kubernetes cluster integrated with your Docker runtime. This is great for development and testing purposes.Steps to enable Kubernetes in Docker Desktop:
Install Docker Desktop (if not already installed) for your OS. Ensure it’s updated to a recent version. Docker Desktop comes with Kubernetes support on Windows and Mac (and on Windows it uses WSL2 backend).
Enable Kubernetes: Open Docker Desktop’s settings/preferences. Navigate to the “Kubernetes” section and check the option “Enable Kubernetes”. Docker will start installing and configuring a single-node Kubernetes cluster internally.
On enabling, Docker might prompt to reset the Kubernetes cluster if it was enabled before. Accept and wait a few minutes.
Configure kubectl: Docker Desktop installs the kubectl command-line tool (or you can install it separately) and merges the Kubernetes context automatically. Ensure your kubeconfig context is set to Docker Desktop’s cluster. You can check by running:
kubectl config get-contexts
kubectl config use-context docker-desktop
The context name docker-desktop is typically configured for you when enabling Kubernetes in Docker Desktop.
Verify the cluster is running: Run a simple command like:
kubectl get nodes
You should see one node (often named docker-desktop) in the Ready state. Also try:
kubectl get services
By default, you’ll see the kubernetes Service in the default namespace (an internal cluster IP for the API server).
If everything is correct, you now have a functioning Kubernetes cluster. Docker Desktop’s Kubernetes is a single-node cluster, meaning the control plane and worker roles are all on the one Docker VM.Troubleshooting Kubernetes startup in Docker Desktop:
If Kubernetes does not start (e.g., it’s stuck in “starting” state for a long time), try restarting Docker Desktop. Sometimes resetting the Kubernetes cluster (via the Docker Desktop GUI -> Kubernetes -> "Reset cluster") can resolve issues if the state is corrupted.
Ensure your Docker resources (CPU/memory) are sufficient. Docker Desktop settings allow you to allocate resources. Kubernetes in Docker needs at least ~2 CPUs and 2GB RAM to function smoothly​
MINIKUBE.SIGS.K8S.IO
. If it’s been given too little memory, it might not schedule the internal pods.
On Windows, if using WSL2, make sure the Kubernetes context is pointing to the Docker Desktop’s Kubernetes and not trying to use some other context. Also, check that WSL2 has the needed updates (Docker’s documentation lists some specific Windows version requirements).
If kubectl commands hang or refuse to connect, check that Docker Desktop’s Kubernetes is showing as running. It may help to run docker info and see if Kubernetes is enabled.
Sometimes you might encounter image pull issues (e.g., the internal “pause” image can’t be fetched due to network). Ensure that Docker can pull images from the internet. If you are behind a proxy, configure Docker’s proxy settings so Kubernetes can pull required images.
Firewall: On Windows, the Kubernetes cluster network might be blocked by local firewall. Typically, Docker handles this, but if you have third-party firewall software, ensure the Docker VM’s network connections are allowed.
Using kubectl with Docker Desktop: The kubectl CLI gets configured automatically by Docker Desktop. If you have a separate kubectl installed, ensure it’s the same version range as the Kubernetes server (Docker Desktop often uses a specific Kubernetes version). You can always check versions with kubectl version --short. In general, minor version skew of +1/-1 is supported by kubectl.Once you have kubectl get nodes showing the node, you are ready to deploy applications on this local cluster.
3. Nginx Deployment with a NodePort Service
Let’s deploy a simple Nginx web server in our cluster and expose it to our host machine using a NodePort service. A NodePort service opens a specific port on the node (in this case, our Docker Desktop VM) to allow external access to the service.We’ll create two Kubernetes YAML manifest files: one for the Deployment and one for the Service (or combine them in one file separated by ---). Below are the manifests.Deployment YAML (nginx-deployment.yaml):
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3  # run 3 Nginx pods for high availability
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: daraymonsta/nginx-257:dreamteam
        ports:
        - containerPort: 80
Let’s break down this deployment:
We set replicas: 3 to run three copies of the Nginx pod. This ensures if one pod goes down, we still have others serving, and it demonstrates load-balancing.
The selector and template.metadata.labels ensure the Deployment manages pods labeled app: nginx.
The container uses the image daraymonsta/nginx-257:dreamteam (as specified in the task). This is presumably a custom image (perhaps a modified Nginx with specific content or configuration). If that image is not accessible or you prefer, you could use a standard nginx:latest image – but we’ll follow the given one.
We expose container port 80 (the Nginx default HTTP port). This is the port Nginx listens on inside the container.
Apply this Deployment with: kubectl apply -f nginx-deployment.yaml. Kubernetes will pull the image and create 3 pods. You can check the pods status with:
kubectl get pods -l app=nginx
This uses a label selector to list only pods with app=nginx. You should see 3 pods, and after pulling the image, their status should be Running. If a pod crashes or image pull fails, describe the pod (kubectl describe pod <pod-name>) to see events. A common issue is image pull error (e.g., if the image name is wrong or you need Docker Hub credentials). For this custom image, ensure it’s publicly accessible or you have logged in (you can use kubectl create secret docker-registry if it needed credentials).Now, to expose these Nginx pods externally, create a NodePort Service.Service YAML (nginx-service.yaml):
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
    - port: 80        # Service port (cluster internal)
      targetPort: 80  # Container port to route to
      nodePort: 30001 # NodePort on the node VM
Here, we define a Service named “nginx-service”. Key points:
spec.type: NodePort means this service will allocate a port on the node (in the 30000-32767 range). We explicitly choose 30001 for clarity (the task suggests exposing on 30001). If we left it blank, Kubernetes would auto-assign a random port in that range.
The selector matches the pods with label app: nginx (so it will route traffic to the 3 Nginx pods we deployed).
Under ports, port: 80 is the service’s cluster-internal port, and targetPort: 80 is where the service forwards traffic on the pods (the containerPort we exposed). Often the service port and container targetPort are the same for simplicity, as we do here (both 80).
nodePort: 30001 opens port 30001 on the Docker Desktop VM. This means any traffic hitting port 30001 on your host (Docker VM) will be forwarded to the service, which in turn balances it to the Nginx pods.
Apply the service: kubectl apply -f nginx-service.yaml. Now verify the service:
kubectl get service nginx-service
It should show something like:
NAME            TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
nginx-service   NodePort   10.104.205.12   <none>        80:30001/TCP   1m
This indicates the service has cluster IP 10.104.205.12 (random internal IP) and that it’s exposing 80 via node port 30001.Now, you can test access. Because this is Docker Desktop, the “node” is essentially your local machine. On Mac/Windows with Docker Desktop, you can often access the NodePort via localhost:30001. However, note that Docker Desktop uses an internal VM. In recent versions, Docker proxies the ports, so http://localhost:30001 on your machine should indeed reach the Nginx service. Alternatively, you might use the VM’s IP. Docker Desktop for Mac/Win typically maps the NodePort to localhost.Open a web browser and go to http://localhost:30001/. You should see the Nginx welcome page or the content served by that daraymonsta/nginx-257:dreamteam image. If it’s a custom page (perhaps it’s a “Welcome Dream Team” page or similar), that confirms your specific image is running.Troubleshooting Nginx accessibility:
If you can’t access localhost:30001, first ensure the pods are running (kubectl get pods). If the pods are CrashLooping, fix that issue first (maybe the image entrypoint fails).
If pods are okay, ensure the service is correct: kubectl describe svc nginx-service should show that endpoints are assigned (it will list the pod IPs under Endpoints). If Endpoints is empty, the label selector might not match. Ensure your pods have the label app: nginx (check with kubectl get pods --show-labels). If you misspelled the label in either the Deployment or Service, the service won’t pick up pods.
If Endpoints are there but localhost:30001 isn’t responding, consider that on Docker Desktop, the NodePort is accessible via localhost. On a Linux environment (Minikube or Docker driver), you might need to use the node’s IP. For Docker Desktop, try curl localhost:30001. If that doesn’t work on Windows, you might need to enable the port in Windows firewall.
Also verify that Docker Desktop Kubernetes is running in docker and not using another context. kubectl cluster-info can show the endpoints.
If you see a response but it’s not the expected content, perhaps the custom image serves on a different port or path. Double-check if daraymonsta/nginx-257:dreamteam is indeed listening on port 80. (Our config assumes it does, as it’s presumably an Nginx derivative.)
At this point, you have a robust Nginx deployment. You can test Kubernetes’ self-healing by killing a pod:
kubectl delete pod -l app=nginx --force
(deleting by label with --force for demonstration). The Deployment’s ReplicaSet will immediately create a new pod to maintain 3 replicas. If you refresh the browser during this, you might not even notice, because the service load-balances across pods and one pod’s termination simply shifts traffic to others. This zero-downtime behavior is a big benefit of using Deployments with multiple replicas.Tip: You can also scale this deployment easily (without downtime). We’ll explore scaling more in the next section.
4. Observing Kubernetes Behaviors (Scaling and Self-Healing)
Now that we have a running deployment, let’s explore some dynamic behaviors of Kubernetes:
Pod self-healing: As mentioned, if a pod in a Deployment is deleted or crashes, Kubernetes will create a new one automatically. You can observe this by listing pods and deleting one:
kubectl get pods -l app=nginx
kubectl delete pod <one_of_pod_names>
After deletion, run kubectl get pods -l app=nginx again. Initially, you’ll see one less pod, but within seconds a new pod with a new name should appear, bringing the count back to 3. The ReplicaSet controller noticed the pod went away and launched a replacement​
SEMATEXT.COM
. This demonstrates Kubernetes’ self-healing capability. In production, if a node went down or a pod process died, users would still hit the service and be routed to remaining pods, while Kubernetes works to recover the lost pods on a healthy node.
Scaling replicas dynamically (zero downtime): Suppose our Nginx deployment needs to handle more load. We can scale out the number of pod replicas on the fly. There are multiple ways to scale:
kubectl scale command: This is a direct way to scale a deployment or replicaset. For example:
kubectl scale deployment nginx-deployment --replicas=5
This tells Kubernetes to increase the nginx-deployment to 5 replicas. The Deployment controller will create 2 new pods (since we had 3, it adds up to 5). You can watch kubectl get pods -l app=nginx -w (watch mode) to see new pods spawning. The new pods will be added to the service endpoints automatically. If you refresh your browser or use a load-testing tool, you can see traffic being spread to more pods. This scaling does not cause downtime – existing pods continue serving while new ones come up. Similarly, you could scale down with the same command (e.g., to 2 replicas); Kubernetes will terminate excess pods.
Editing the Deployment (imperatively): You can run kubectl edit deploy nginx-deployment. This opens the deployment YAML in your default editor. Changing the replicas: 5 in the spec and saving will update the deployment. This is essentially doing the same as the scale command, but via editing the live object.
Applying a modified YAML (declarative): You could also edit your nginx-deployment.yaml file to have replicas: 5 and then run kubectl apply -f nginx-deployment.yaml again. Kubernetes will see the deployment spec changed and perform a rolling update to match it. In the case of just replica count change, it’s a quick scale operation.
All these methods achieve a graceful scaling. Kubernetes ensures that at any point, at most one pod might be terminating during a downscale (default maxUnavailable = 1 for Deployments), and new pods are ready before old pods are removed during upscale/downscale events to avoid dropping below the desired count. In our Nginx example, adding pods doesn’t disrupt existing ones at all.
Zero downtime deployment updates: Although not explicitly in the tasks, it’s worth noting that if you update the container image of a Deployment, Kubernetes will do a rolling upgrade: incrementally launch new pods with the new image and terminate old ones, ensuring some overlap to avoid downtime. This can be configured (e.g., rolling update strategy). By default, it’ll spawn one new pod, then kill one old pod, etc., as configured by maxSurge and maxUnavailable.
Cleaning up resources: It’s important to delete Kubernetes resources when they’re no longer needed, especially in a dev environment, to avoid consuming cluster resources. To delete our Nginx setup, you can run:
kubectl delete -f nginx-deployment.yaml
kubectl delete -f nginx-service.yaml
This will remove the Deployment (and thus its ReplicaSet and pods) and the Service. Alternatively, since we labeled everything with app=nginx, you could do kubectl delete deploy,svc -l app=nginx to delete all deployments and services with that label. Always ensure you specify the object kind (deployment, service, etc.) when deleting by label to avoid accidentally deleting other things that might share a label.
Troubleshooting scaling: Scaling is usually straightforward. If kubectl scale doesn’t seem to do anything, check if the Deployment has any conditions preventing scaling (rare). Also ensure you are targeting the correct resource name/namespace. If new pods are created but stuck in Pending state, that could mean insufficient cluster resources (not an issue in Docker Desktop usually, but in small Minikube clusters it can be). If a pod is Pending, describe it to see if it’s waiting for a volume or node port conflict or some other reason. In our case, node ports are specified; if you tried to scale another deployment with the same node port, it would conflict (Kubernetes would refuse to schedule the service). But scaling pods under one service doesn’t create new node ports; they all share the same service node port.After exploring, you can scale back to 3 or down to 0. In fact, scaling to 0 is a way to temporarily pause a deployment (zero replicas means no pods). The service will have no endpoints, so traffic to it will fail, but the Deployment object still exists and can be scaled up later. This is a quick way to stop an app without deleting the deployment definition.
5. Deploying a Node.js “Sparta” Test App (with MongoDB)
Now we’ll deploy a two-tier application: a Node.js application (the “Sparta” test app) and a MongoDB database it connects to. We’ll use Kubernetes to run both components and wire them together. The end goal is to have the Node app reachable via a NodePort and the Node app talking to MongoDB via an internal service.Setting up the Node.js app Deployment: Assuming we have a Node.js app packaged into a Docker image (perhaps the test app’s image is available or you built it as myrepo/sparta-app:v1), we create a deployment for it. Let’s say the Node app listens on port 3000 inside the container (common for Node apps). We also need to provide it with the MongoDB connection info via environment variables.Example Deployment YAML for the Node app:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sparta-node-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: sparta-node
  template:
    metadata:
      labels:
        app: sparta-node
    spec:
      containers:
      - name: sparta-app
        image: myrepo/sparta-app:v1   # replace with the actual image name
        ports:
        - containerPort: 3000
        env:
        - name: MONGO_URL
          value: "mongodb://mongo-service:27017/sparta" 
A few things to note:
We run 3 replicas of the Node app for high availability.
The container exposes port 3000. (Adjust this if your app uses a different port.)
We set an environment variable MONGO_URL (this name might differ based on how the app expects configuration; adapt to your app). In this example, the app will use the connection string mongodb://mongo-service:27017/sparta. Here, mongo-service will be the hostname of our MongoDB service within the cluster, port 27017 is the default MongoDB port, and “sparta” could be the database name. Alternatively, we might pass separate env vars like DB_HOST, DB_NAME, etc., depending on app design.
Deploying and verifying the Node app: Apply this deployment with kubectl apply -f sparta-node-deployment.yaml. After a few moments, do kubectl get pods -l app=sparta-node. The pods should be Running. If not, troubleshoot:
If ImagePullBackOff, ensure the image name is correct and accessible (push it to Docker Hub or a registry Docker Desktop can access).
If CrashLoopBackOff, use kubectl logs <pod> to see the Node app’s output. Perhaps it’s failing to connect to Mongo (which we haven’t set up yet!). It might continuously retry connection – which is okay for now until we add Mongo.
We won’t be able to fully test the Node app until the database is up, but we can expose the Node app service in parallel.Exposing the Node app via NodePort: We create a service similar to Nginx:
apiVersion: v1
kind: Service
metadata:
  name: sparta-node-service
spec:
  type: NodePort
  selector:
    app: sparta-node
  ports:
  - port: 3000
    targetPort: 3000
    nodePort: 30002
This will route external port 30002 to the Node app pods (port 3000 in the container). Apply this (kubectl apply -f sparta-node-service.yaml). Check kubectl get svc sparta-node-service to confirm it’s assigned the nodePort 30002 as specified.Now you have the Node app set up on the Kubernetes side, but it will error until MongoDB is running. Let’s deploy MongoDB.Adding a MongoDB database deployment: For MongoDB, we can use the official image mongo:4.4 (or latest). We’ll start with a simple deployment of a single Mongo pod (for a toy app, one DB instance is fine; in real scenarios, you’d consider a StatefulSet for replicas). We will also set up a service for MongoDB so the Node app can find it by name.Mongo Deployment YAML:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongodb
        image: mongo:4.4
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_DATABASE
          value: "sparta"
This will run a MongoDB container. We set MONGO_INITDB_DATABASE to "sparta" just as an example to automatically create a database on first run (the Sparta app might expect a certain DB to exist; this env var causes Mongo to create an initial DB by that name). We’re not setting any root password or user here for simplicity (Mongo by default in dev mode will allow connections without auth inside the container). For a production-grade setup, you’d set MONGO_INITDB_ROOT_USERNAME and ...PASSWORD env vars to enable authentication, and then your app would use credentials. But to keep things simple, we’ll run it open (only accessible inside cluster anyway).Now, the Service for Mongo:
apiVersion: v1
kind: Service
metadata:
  name: mongo-service
spec:
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017
    # ClusterIP service (default type) - no external exposure
We don’t need to specify a type; by default it will be ClusterIP, which means it’s only accessible within the cluster (which is what we want for a database). The service name “mongo-service” is what we used in the Node app’s MONGO_URL. Kubernetes DNS will resolve mongo-service to this service’s cluster IP, and pods can connect on port 27017 to reach the MongoDB.Apply these: kubectl apply -f mongo-deployment.yaml and kubectl apply -f mongo-service.yaml. Check that the Mongo pod comes up (kubectl get pods -l app=mongo). If it’s Running, good. If it’s CrashLooping, check logs (kubectl logs <mongo-pod-name>) – sometimes Mongo might crash if persistent storage is not stable (we will address storage on Day 2). But usually, it should run in memory with ephemeral storage for now.At this point, the Node app pods should automatically be able to connect to Mongo (assuming the app is configured to use the MONGO_URL or similar environment we provided). The Node app likely tries to connect on startup. Now that Mongo is live, those connection attempts should succeed. Check the Node app pod logs again (kubectl logs <sparta-pod>). You might see messages like “Connected to MongoDB” or the app listening on port, etc., which indicates things are working. If the app still crashes, it might be because it started before Mongo was up and didn’t retry. In that case, you might want to add a simple retry logic or liveness probe (beyond scope here). But let’s assume it recovers or was still running and now has connected.Testing the full application: You have:
Node app service on NodePort 30002
Nginx still on 30001 (if you kept it) – that was separate, not needed for the Node/Mongo app, unless the Node app is actually served via Nginx (not in this case, presumably).
Mongo is internal only.
Open a browser or use curl: http://localhost:30002/ (or whatever endpoint the Sparta app provides, e.g., maybe / or /status or some test path). If all is well, you should get a response from the Node app. This Node app likely interacts with Mongo – for example, it might have a route that writes to the DB or reads from it. Try to exercise its functionality to confirm the DB connection works (some test endpoints might be described in the Sparta app documentation).Troubleshooting the Node.js and Mongo setup:
If the Node app is not reachable on 30002, check kubectl get svc sparta-node-service to confirm the NodePort. On Docker Desktop, again, localhost:30002 should forward to it.
If you get an error from the Node app (like it returns an application error), check the Node pod logs for stack traces. It could be unable to query the database or some internal error.
If Node logs show it cannot connect to Mongo (e.g., connection refused), check that within the Node container, DNS is resolving correctly. You can exec into a Node pod (kubectl exec -it <node-pod> -- ping -c3 mongo-service) to see if it can reach the Mongo service. If ping works (DNS resolved, though note that some images might not have ping installed; alternatively nslookup mongo-service using busybox), then DNS is fine. In that case, maybe Mongo isn’t ready. Ensure the Mongo pod is running and not restarting.
If the Mongo pod is CrashLooping, as mentioned, it might be due to no persistent storage on restart. On initial deploy, it should run. We will add persistence soon.
Ensure the app’s env var matches the service name. If the Node app is looking for MONGO_HOST or something, our MONGO_URL might not be used. We assumed a generic connection string env. Adapt to your app’s config needs. You can pass multiple env vars (like DB_HOST = mongo-service, DB_PORT = 27017, etc.).
Now, we have a basic two-tier architecture running on Kubernetes. It’s a good practice to organize your manifests in a Git repository. For example, you might create a directory structure:
k8s-manifests/
  day1/
    nginx-deployment.yaml
    nginx-service.yaml
    sparta-node-deployment.yaml
    sparta-node-service.yaml
    mongo-deployment.yaml
    mongo-service.yaml
And so on. We’ll refine this setup on Days 2 and 3 with persistence and autoscaling.Before moving on, commit your YAML files to a Git repo (if you have one set up) and document what you did in the README (including any issues encountered and how you solved them). Capturing command outputs (logs, kubectl get all output) in your documentation is helpful for debugging and for others to understand the state.At this stage, Day 1 deliverables would include: the cluster up and running (Docker Desktop with Kubernetes), an Nginx deployment reachable on localhost:30001, and the Sparta Node.js app with MongoDB reachable on localhost:30002 (assuming the test app exposes something at that port). We also demonstrated scaling and self-healing. All YAMLs and a write-up should be in your repo.
Day 2: Persistent Storage and Autoscaling

1. Persistent storage for the database (MongoDB)
On Day 1, we deployed MongoDB with its storage in the container’s filesystem (which in Kubernetes by default is ephemeral storage on the node). This means if the Mongo pod restarts or is rescheduled, all data would be lost, which is not acceptable for a database in most cases. To fix that, we use Kubernetes PersistentVolumes (PV) and PersistentVolumeClaims (PVC) to provide durable storage.A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned for use by pods​
KUBERNETES.IO
. It exists independently of any individual pod’s lifecycle. In other words, a PV is like a disk (could be backed by cloud storage, NFS, or local disk) that remains even if pods come and go. A pod gets storage by asking for a PersistentVolumeClaim, which is essentially a request for a PV of a certain size and type​
KUBERNETES.IO
.Kubernetes decouples the request (PVC) from the actual resource (PV) so that developers can just ask for storage and admins can provide different types of storage without the pod needing to know the details. The cluster will bind a PVC to a suitable PV if one is available.In our case, we’ll create a hostPath PV (since on Docker Desktop, we can use the host filesystem for simplicity). Docker Desktop’s Kubernetes might also have a default storage class that dynamically provisions hostPath volumes. But we can demonstrate with a static PV/PVC.Create a PersistentVolume YAML (mongo-pv.yaml):
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mongo-pv
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/data/mongo"
  persistentVolumeReclaimPolicy: Retain
This defines a PV:
Capacity 100Mi (just for testing, 0.1GB). Ensure your Docker Desktop VM has that space free (it should).
AccessModes: ReadWriteOnce (means it can be mounted by one node at a time in read-write – that’s fine because we only have one node and one pod for Mongo).
hostPath: "/data/mongo" on the node – this is a directory on the node’s filesystem. Docker Desktop’s node is a VM where /data/mongo will be created. (Using hostPath ties us to that environment, but it’s simplest for demo. On a real cluster, you’d use network storage or cloud volumes.)
ReclaimPolicy: Retain – this is important. It means if we delete the PVC later, the PV is not automatically deleted. The data remains on disk until an admin manually cleans it or reassigns it. We choose Retain because one of our tasks is to remove the PVC while retaining data – we’ll see that in action.
Next, a PersistentVolumeClaim to actually use this PV (mongo-pvc.yaml):
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongo-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi
  volumeName: mongo-pv
Here:
We request 100Mi storage, RWO access (matching what PV offers).
By specifying volumeName: mongo-pv, we explicitly bind to the PV we created. (Alternatively, one could rely on label matching or storageClass binding, but explicit is fine here since we know the PV name.)
If we had a storageClassName defined on the PV and PVC, binding can also happen by class. In this simple case, leaving storageClassName blank and specifying volumeName does a manual binding.
Apply the PV and PVC:
kubectl apply -f mongo-pv.yaml
kubectl apply -f mongo-pvc.yaml
Check kubectl get pv mongo-pv and kubectl get pvc mongo-pvc. The PV’s STATUS should change to Bound (bound to the PVC)​
SPOT.IO
. The PVC’s status becomes Bound as well, indicating it successfully claimed the PV.Now we need to modify the MongoDB Deployment to use this storage. We’ll add a volume and volumeMount to the pod spec:Edit the mongo deployment (or create a new YAML mongo-deployment-persistent.yaml):
spec:
  template:
    spec:
      containers:
      - name: mongodb
        image: mongo:4.4
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_DATABASE
          value: "sparta"
        volumeMounts:
        - name: mongo-storage
          mountPath: /data/db
      volumes:
      - name: mongo-storage
        persistentVolumeClaim:
          claimName: mongo-pvc
Changes:
Added a volumeMounts entry to mount our volume at /data/db (that’s where MongoDB stores its database files inside the container by default).
Added a volumes section referencing mongo-pvc. This tells Kubernetes to attach the PV (via the claim) to the pod.
Apply the updated deployment. Since we changed the pod spec, Kubernetes will terminate the old Mongo pod and create a new one (rolling update with maxUnavailable=0 or 1). The new pod will start MongoDB and now it will be writing data to the host path /data/mongo on the node, which is persisted.We should verify persistence:
Connect to Mongo (if you have the Mongo client on host or exec into the pod and use the mongo shell) and add some sample data. For instance:
kubectl exec -it <mongo-pod> -- mongo sparta --eval 'db.test.insert({status:"alive"})'
This inserts a document into a test collection. Then exit.
Delete the Mongo pod: kubectl delete pod -l app=mongo. The Deployment will start a new pod. When the new pod comes up, it will mount the same /data/mongo path. Check the data:
kubectl exec -it <new-mongo-pod> -- mongo sparta --eval 'db.test.find().pretty()'
You should see the document you inserted. This confirms that the data persisted across pod restarts because it was stored on the PV which outlived the first pod.
Kubernetes PV/PVC architecture: With this setup, the PersistentVolume is a cluster resource, and the PersistentVolumeClaim is a way for the pod to use that resource. The pod references the PVC, which is bound to the PV. The data is stored on the node’s filesystem (since we used hostPath). Note that if this single Docker Desktop node VM were destroyed, the data goes with it. In a cloud scenario, typically the PV would be something like an AWS EBS volume which could potentially be reattached to a new node if the old node dies. The concept, however, is that the PV is independent of the pod’s lifecycle – “PVs belong to the cluster and exist independently of pods. Any data on a PV continues to exist even if the pod is deleted.”​
SPOT.IO
. The PVC is what allowed the pod to “claim” that storage and mount it.
SPOT.IO
In our case, if we delete the PVC mongo-pvc, the PersistentVolume mongo-pv would enter a “Released” state (the claim is gone but data remains) because we set reclaimPolicy: Retain. The PV is not automatically deleted or reused; it holds the data until an admin intervenes​
K21ACADEMY.COM
. This is a safeguard to prevent accidental data loss. We’ll try this in a moment.PVC removal while retaining data: Let’s simulate the scenario: remove the PVC but keep the PV (and data). Ensure the Mongo pod is not running or using it first (you can scale down or delete the mongo deployment before this, so it releases the volume). Then:
kubectl delete pvc mongo-pvc
Now run kubectl get pv mongo-pv. You will see its STATUS is Released (meaning it’s no longer bound to a claim) and it is not available for another claim yet because the data might still be there and it’s in Retain state. If you describe the PV, Kubernetes will note that manual intervention is needed to reuse it (you’d have to either wipe the data or manually clear the released condition). The important thing is the data on disk wasn’t deleted when the PVC was deleted​
K21ACADEMY.COM
. If we had set reclaimPolicy: Delete, then deleting the PVC would have also deleted the PV and, by extension, likely wiped the storage (e.g., deleted the cloud disk or removed the files for hostPath). Retain gives you the chance to recover data or rebind the PV later.For our project, we likely will keep the PVC around normally. The task “Removing PVC while retaining data in Persistent Volume” is likely a test to ensure you used Retain and understand the consequence. We demonstrated that. We would then re-create the PVC (or set the existing PV to Available and bind a new PVC) to get the pod running again. Typically, one might use kubectl patch pv mongo-pv -p '{"spec":{"claimRef": null}}' to make it Available again, then re-create a PVC to rebind. But be careful with that – data still corresponds to the old claim.In practice, if you delete a PVC accidentally but want to keep data, with Retain you can create a new PVC that manually binds to that PV (by name, like we did with volumeName earlier). So you can recover.For now, to continue, it’s simpler not to delete the PVC at all, so our Mongo stays bound.We should update our git repository documentation (README) with a diagram of how PV/PVC attach to the pod. A simple diagram might illustrate: Pod -> PVC -> PV -> host storage. In absence of a ready-made diagram here, just describe it: The pod spec has a volume using the PVC, the PVC is bound to a specific PV, and the PV resides on a node (or external storage). This indirection allows pods to be rescheduled to other nodes while still referring to the same storage (Kubernetes will attach/mount the volume on the new node).Summary: We have now ensured that our MongoDB data persists across pod restarts. If the MongoDB container restarts or even if we upgrade the MongoDB Deployment (rolling out a new image), the data on the PV remains intact and the new pod uses it. This meets the requirement of data retention.
2. Autoscaling: research and implementation
Kubernetes autoscaling happens at multiple levels:
Horizontal Pod Autoscaler (HPA): Scales the number of pod replicas for a deployment or statefulset based on observed metrics (like CPU, memory, or custom metrics)​
KUBERNETES.IO
.
Vertical Pod Autoscaler (VPA): Adjusts the resource requests/limits for containers in pods (i.e., “vertical” scaling by giving pods more CPU/memory or less)​
SPOT.IO
.
Cluster Autoscaler: Adds or removes worker nodes in the cluster based on pending pods or resource utilization​
SPOT.IO
.
These three can work in tandem – for example, HPA scales out pods, and if the cluster runs out of capacity, Cluster Autoscaler can add more nodes. VPA can right-size pods over time. In our environment (Docker Desktop), we only have one node and cannot add nodes, so cluster autoscaler isn’t applicable. We’ll focus on HPA (and mention VPA conceptually).Horizontal Pod Autoscaler (HPA): HPA is implemented as a Kubernetes controller that periodically (default every 15s) checks the metrics of pods in a target deployment and adjusts the replica count up or down to maintain a target usage​
SPOT.IO
​
SPOT.IO
. For example, you can set an HPA to target 50% CPU utilization. If pods exceed that, HPA will scale out (add replicas) until average CPU is ~50%. If they’re under-utilized, it will scale in (remove replicas) down to a minimum you specify. HPA can also scale on custom application metrics or even external metrics, but CPU and memory (resource metrics) are common.Vertical Pod Autoscaler (VPA): VPA monitors the actual resource usage of containers and can recommend or automatically adjust the CPU/memory requests of pods​
SPOT.IO
. If a pod consistently uses more than its requested resources, VPA can increase the request so that next time the scheduler places it, it gets a guaranteed slice of CPU/memory. VPA can, however, conflict with HPA if both are scaling CPU – you typically don’t run HPA on CPU and VPA on the same deployment simultaneously, because one is changing replica counts while the other changes individual pod sizes. In practice, VPA often runs in a recommendations mode, and is applied to slower-changing workloads.Cluster Autoscaler: runs typically as a separate component (often as a deployment in the cluster or an external process) and uses cloud provider APIs to provision new VMs (nodes) when the scheduler can’t place pods due to lack of resources​
SPOT.IO
. It also scales down by removing underutilized nodes (after rescheduling those pods elsewhere)​
SPOT.IO
. In managed services like EKS/GKE, cluster autoscaler can be enabled to automatically manage node pools. In Docker Desktop or minikube, we can’t really simulate this, as there’s no cloud to provide new nodes. So we skip it here.Now, let’s implement an HPA for our Node.js app (the Sparta app). We want it to scale between 2 and 10 replicas based on load.Prerequisite: The metrics server. Kubernetes HPA (for CPU/memory) relies on metrics API (metrics-server). Some clusters (like GKE) have it by default. Docker Desktop’s Kubernetes usually has metrics-server installed by default (you can check: kubectl get deployment metrics-server -n kube-system). If not, you’d install it (there’s a YAML or helm chart for metrics-server). Without metrics, HPA won’t work (HPA will stay in “Unknown” state for metrics). Assuming it’s in place, proceed.We set the HPA to maintain ~50% CPU usage per pod as target (just an example; or we could say scale if CPU > 50%). Suppose our Node app is CPU intensive under load.Create an HPA YAML (sparta-hpa.yaml):
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: sparta-node-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: sparta-node-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
Explanation:
scaleTargetRef points to the deployment we want to autoscale (our Node.js deployment).
minReplicas = 2, maxReplicas = 10 sets the range.
Metrics: using autoscaling/v2 API, we specify a CPU target. averageUtilization: 50 means if the pods’ average CPU usage > 50% of their request, HPA will scale up, and if it’s consistently <50%, scale down (not below 2 though). This assumes the pods have CPU requests set. If not, HPA uses some default or might not work; it’s best to have a cpu request in the Node deployment spec, e.g., resources: requests: cpu: 100m etc., so that 50% of that = 50m usage triggers scale.
Apply this: kubectl apply -f sparta-hpa.yaml. You can check kubectl get hpa sparta-node-hpa. Initially it might say unknown or 0% CPU if idle. If your app is idle, HPA will scale it down to minReplicas (2). We set min 2, so it might actually scale down from 3 to 2 right away, since target is 50% and current usage is ~0%. Don’t be surprised if you see it go to 2 replicas after a while.Generating load to test HPA: To see it scale up, we need to load the app so CPU rises. There are a few ways:
Run a load test from your host or a container. You could use a tool like ab (ApacheBench) or hey or simply a looped curl. For example: for i in {1..1000}; do curl -s http://localhost:30002/ > /dev/null & done to send a bunch of parallel requests. If the Node app does some CPU work per request, this might increase CPU usage.
Alternatively, exec into one of the Node pods and run a stress test inside (less ideal). Better to generate external load.
Monitor the HPA: kubectl get hpa sparta-node-hpa -w (watch). It will show current replicas, target, current usage%. It typically needs a couple of metrics samples (metrics-server scrapes every 10s or so, HPA checks every 15s). After a short time under heavy load, you should see CPU% go high and HPA will increase replicas (maybe to 4, then 6, etc., up to 10 if needed). Also watch the pods: kubectl get pods -l app=sparta-node -w to see new pods spawn.If the load stops, after a cooldown period (stabilization window by default a few minutes), HPA will scale back down, but not below 2 (min).Troubleshooting HPA:
If kubectl get hpa shows <unknown> or no metrics, ensure metrics-server is running. You might need to install it (in Minikube, minikube addons enable metrics-server; in Docker Desktop, it should be included).
If HPA scales up to max but performance is still low, you might be hitting node limits (in Docker Desktop, only 2 CPUs given). Our single-node can only run so many pods effectively. It’s just to demonstrate.
If pods have no resource requests, HPA can still scale based on actual usage (it defaults to using some base, but it’s recommended to set requests).
Remember HPA won’t scale faster than one step per 15 seconds (and scale down is more conservative). So give it time.
Autoscaling types recap: HPA deals with replicating pods (scale out/in)​
KUBERNETES.IO
, VPA deals with sizing pods, Cluster Autoscaler deals with adding/removing nodes. They each address different levels of scaling:
HPA = handle variable load by adjusting concurrency (more pods).
VPA = ensure each pod has appropriate resources to avoid being underpowered or wasting resources.
Cluster Autoscaler = ensure the cluster has enough machines to run the pods demanded (and not too many idle machines).
For our guide, implementing HPA is sufficient. In documentation, mention perhaps that VPA could be used for the MongoDB to give it more memory if it grows, etc., but we won’t implement that now.One more thing: after testing, you might remove the HPA if not needed by kubectl delete hpa sparta-node-hpa. Or leave it – it’s okay.Now we have:
Node app can auto-scale between 2 and 10 pods on CPU load.
Mongo doesn’t auto-scale (that would be a StatefulSet usually, or one might scale read replicas, but not in scope).
We preserved data on Mongo via PV.
Before finishing Day 2, commit all new YAMLs (PV, PVC, modified deployment, HPA) to the repo and update your README with what you did. Possibly include a screenshot or CLI output showing HPA scaling events (to show it working). Document any issues (for example, “Had to enable metrics server addon to get HPA metrics” or “Noticed that scaling beyond 6 pods on Docker Desktop overwhelmed the CPU”).We also achieved the task of removing PVC while retaining data: as explained, by setting reclaimPolicy to Retain, deleting the PVC did not delete the PV (data). We should document this test in our log, showing PV in Released state after PVC deletion. If this were a production scenario, we’d keep the PVC – the exercise was to illustrate the Retain policy effect.At this point, Day 2 deliverables include the Kubernetes manifests for PV/PVC and HPA, evidence of data persistence (e.g., logs or a note that data remained after pod restart), and the HPA behavior verification (perhaps a small note of CPU usage and scaling).We can also draw a simple architecture diagram including the persistent storage. For example, a diagram with: Node.js Deployment (HPA enabled) -> Service -> (users), Node.js pods -> Mongo Service -> Mongo Pod -> PV -> host disk. This shows how a PV attaches to the Mongo pod. Ensure to label that PV is 100Mi hostPath. A visual can help solidify understanding that the volume is outside the pod’s lifecycle but within the cluster.
Day 3: Deploying on a Cloud VM with Minikube and Advanced Networking

Now that everything works on Docker Desktop (local), we’ll simulate a more production-like environment by setting up Kubernetes on a cloud instance (e.g., an Ubuntu 22.04 VM on AWS or Azure). We’ll use Minikube to set up a single-node cluster on that VM. Additionally, we’ll practice deploying multiple apps and exposing them with different methods (NodePort, LoadBalancer, Ingress/Reverse Proxy).
1. Minikube setup on a cloud instance (Ubuntu 22.04 LTS)
Setting up the VM: Provision a VM on your cloud of choice. Ensure it has at least 2 CPUs and 4GB RAM for smooth operation (more if running multiple apps). Ubuntu 22.04 is the OS. Open necessary firewall ports: we’ll need port 22 (SSH), port 80 (to expose our apps via HTTP), and possibly NodePort range (30000-32767) or specific NodePort we plan to use (e.g., 30002) for testing. But if we use an Nginx reverse proxy on port 80, we can funnel traffic through port 80 and avoid exposing high-number ports.Install Docker and Minikube dependencies: SSH into the VM and do the following:
Update OS: sudo apt-get update && sudo apt-get upgrade.
Install Docker (Minikube can use Docker driver). For Ubuntu: follow Docker’s official install or simply sudo apt-get install -y docker.io. After install, ensure the docker service is running and add your user to the docker group if you plan to run without sudo.
Install Minikube: You can download the binary:
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube
This places minikube in /usr/local/bin.
Install kubectl:
curl -LO "https://storage.googleapis.com/kubernetes-release/release/`minikube kubectl --version`/bin/linux/amd64/kubectl"
chmod +x kubectl
sudo mv kubectl /usr/local/bin/
(Alternatively, install via snap or apt if available, or get a specific version. Minikube often can download a matching kubectl as shown.)
Start Minikube: We will use the Docker driver (meaning Minikube will create a single-node Kubernetes that runs in a Docker container on this VM). Start it with:
minikube start --driver=docker --memory=4g --cpus=2
Minikube will pull the required images (kicbase image for the node) and set up Kubernetes. This can take a few minutes on first start. If everything succeeds, you’ll get a message that the cluster is running and kubectl is configured to use it.Test with:
kubectl get nodes
It should show one node named “minikube”. Also try kubectl get pods -A to see system pods; all should be running (like kube-system pods, etc.).Networking in Minikube: By default, Minikube on Docker driver will not have a LoadBalancer capability (as there’s no cloud). NodePort services will be accessible on the VM’s IP. For example, if we expose NodePort 30002, to reach it from outside, we use http://<VM public IP>:30002. Ensure your cloud’s security group or firewall allows that port if you go that route.However, often it’s easier to use Ingress or a reverse proxy on port 80. We will use an Nginx reverse proxy on the host to forward traffic to our NodePort services, so that end-users can just hit port 80. This avoids opening many ports and is a simpler URL.Storage in Minikube: Minikube usually comes with a default StorageClass that provisions hostPath volumes on the VM. We already made a PV manually, but we could also let Minikube dynamic provision a hostPath PV by creating a PVC with no specific PV (Minikube’s default StorageClass is “standard” which uses hostPath provisioner). In our case, we’ll stick to our static PV approach for the Mongo DB, or we could convert to using a PVC with storageClass. Either works. Just note, the path on this Ubuntu VM for hostPath might be different. Our manifest uses /data/mongo. We should make sure that path exists and is writable by the container. Minikube’s kubelet likely runs as root, so it can create it.Deploying our applications on Minikube: We will now deploy the same Sparta app (Node.js + MongoDB) on this Minikube, as well as potentially two other dummy applications to practice multiple app deployment.Let’s say we have three applications:
The Sparta Node.js app (with Mongo) – we already have manifests for those.
An additional Nginx (for variety).
Perhaps a simple hello-world app (like Kubernetes’ example or another small web app).
We can reuse our YAML from earlier, with slight modifications if needed for environment differences:
The PV hostPath path might be different if we want; but we can reuse /data/mongo.
NodePort numbers: We used 30001 and 30002 before. We can reuse or pick new. We know we want one NodePort to test directly, one to test via LoadBalancer, etc.
Deploy multiple apps:

App1: Sparta Node.js + MongoDB – We’ll treat this as one “application” (two deployments, one service, plus PVC etc.). Expose via NodePort 30002 (like before). App2: Nginx – Could be the same Nginx we did, or maybe an Apache HTTPD for variety. Let’s stick to Nginx. 3 replicas, NodePort 30001. App3: Another app via LoadBalancer – since Minikube can’t truly provision a cloud LB, we will simulate by using minikube tunnel which creates a local LoadBalancer IP on the host. Alternatively, we use NodePort but run minikube service <name> to access it. To meet the task, let’s do a LoadBalancer service for perhaps a simple app like a guestbook frontend? But that’s complex. Instead, we can convert our Nginx service to LoadBalancer (but we already have NodePort example). Actually, the task said one NodePort, one LoadBalancer, and then use Nginx reverse proxy to route external traffic. Possibly:
Expose Node app via NodePort (30002).
Expose Nginx directly via LoadBalancer (so it would get an external IP through minikube tunnel).
Then Nginx reverse proxy on host to route maybe based on hostname or path to those services.
However, having Nginx itself be both an app and also reverse proxy might confuse. Maybe better:
“Application 1” – Node/Mongo (NodePort).
“Application 2” – say an Echo server or another service (we can use a publicly available simple API, or even reuse Nginx as an app).
“Application 3” – something like Kubernetes Dashboard? But too heavy. Could deploy a dummy hello app (there’s a k8s.gcr.io/echoserver container which just echoes request data). Let’s use that for simplicity:
echoserver Deployment (1 replica, container k8s.gcr.io/echoserver:1.4, exposes port 8080).
Service type LoadBalancer for echoserver on port 8080.
Now we use minikube tunnel. Running minikube tunnel on the VM will require sudo (it creates network interfaces). It will output something like “Entering tunnel for LoadBalancer service...”. When we create a LoadBalancer service, Minikube will allocate an IP (usually from a local range, e.g., 10.96.x.x cluster IP range or sometimes it uses the cluster IP as external IP). The tunnel essentially listens on that IP and forwards to the service.However, since we plan to set up an Nginx reverse proxy, we might not actually need to rely on the LB IP from outside. But we can test it.Let’s do this step by step on the VM:
Deploy Nginx (Deployment + NodePort Service 30001).
Deploy echoserver (Deployment + LoadBalancer Service).
Deploy Node/Mongo (Deployment + PVC/PV + Service NodePort 30002, etc.).
We must ensure ports 30001 and 30002 are open if we want to test NodePort directly. If we plan to only expose via Nginx on 80, we might not open them to public. But for completeness, maybe open them temporarily for testing.Applying YAMLs in Minikube: Use kubectl apply -f for all manifests. They should all come up (check pods). For LoadBalancer service, do minikube tunnel in a separate SSH session (so it keeps running). When the echoserver service is created, kubectl get svc echo-service (for example) will initially show <pending> in EXTERNAL-IP column. After starting minikube tunnel, it should update to some IP (like 10.0.2.15 or something). That IP is actually on the VM itself. You can curl that IP:port from the VM to see if it responds.Minikube tunnel also often requires root to bind to those IPs, so keep it running with sudo.Now, configure Nginx reverse proxy on the VM to route external traffic: We have two apps to expose: for example,
Node app on NodePort 30002
Echo app on LoadBalancer IP (though we could also access it via NodePort if we gave it one, but we gave LB type).
We will configure Nginx (on the host VM, not the one in cluster) to listen on port 80 and proxy based on URL:
http://<VM IP>/node/* -> forwards to localhost:30002 (the NodePort of Node app).
http://<VM IP>/echo/* -> forwards to the echo service.
However, the echo service is on a cluster IP or LB IP which might not be directly reachable externally unless we use the tunnel’s IP. Since we have NodePort to everything as an alternative, maybe simpler: For echo app, instead of LB, we could also create a NodePort to target it (just to use Nginx proxy easily). But since we set up LB to practice, we can still connect to it via tunnel IP from the host. Actually, after running minikube tunnel, the LB IP (let’s call it 10.96.0.200) is accessible on the host. So Nginx on host can proxy to 10.96.0.200:8080 (for echo service).Alternatively, we could skip LB and just use NodePort for echo too, and demonstrate minikube tunnel separately as a concept. The task explicitly said one via LoadBalancer (with tunnel). So we did that for echo. We’ll still route via Nginx by pointing to that LB IP.Install Nginx on Ubuntu VM:
sudo apt-get install -y nginx
Nginx will likely start automatically listening on port 80. We will customize its config. Edit /etc/nginx/sites-available/default (or create new site): For example:
server {
    listen 80;
    server_name _;

    location /node/ {
        proxy_pass http://127.0.0.1:30002/;
        # adjust if the Node app expects no /node prefix:
        rewrite ^/node/(.*)$ /$1 break;
    }

    location /echo/ {
        proxy_pass http://10.96.0.200:8080/;
        rewrite ^/echo/(.*)$ /$1 break;
    }
}
This assumes:
Node app is fine with or without the /node prefix (we strip it when proxying).
The echo server likely just returns what you send. It doesn’t care about path.
After editing, test Nginx config sudo nginx -t. Then sudo systemctl restart nginx.Now, from your local machine, you can hit the cloud VM’s public IP:
http://<VM_PUBLIC_IP>/node/ -> should reach Node app. (If Node app has a web interface, you’ll see it; or maybe a JSON message).
http://<VM_PUBLIC_IP>/echo/ -> should hit the echo server and likely return some HTML with request details.
This setup is essentially acting like an Ingress: Nginx is forwarding to services. In production, you might actually run an Ingress Controller inside the cluster to do this, but here we manually did it on the host.Document Minikube networking: Important points to note:
With NodePort, the service is accessible via NodeIP:NodePort. Our VM had NodeIP = 192.168.49.2 (Minikube’s default VM IP) and we used localhost in Nginx because we’re on the same host. From outside, you either open the NodePort or use a proxy.
With LoadBalancer service and minikube tunnel, Minikube allocated a pseudo external IP and routed traffic from host to service. This is a way to simulate cloud LoadBalancers. The tunnel has to run constantly for the IP to work.
We chose to not directly expose these to the internet but instead funnel through Nginx on port 80, which is often open. This is a common pattern: you might have an Nginx or HAProxy as a gateway.
Clean up: if you stop minikube tunnel, the LB IP stops working. If you shut down the VM or Minikube, all pods go down (unless you set minikube to start on boot, which we’ll address).
Cleanup and management: When done, you can stop the cluster with minikube stop (though we might want it to auto-start on boot). You might also want to configure Minikube to start on boot: One way is to create a systemd service that runs minikube start at boot as root. Alternatively, since it’s one node, maybe just restart manually via ssh if needed. But the task says "Ensuring minikube start happens automatically on instance restart".To do this: Create a systemd unit file /etc/systemd/system/minikube.service:
[Unit]
Description=Minikube
After=docker.service
Requires=docker.service

[Service]
User=root
ExecStart=/usr/local/bin/minikube start --driver=docker
ExecStop=/usr/local/bin/minikube stop
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
Then sudo systemctl enable minikube.service. This will try to start it on boot. Make sure Docker is up before it runs (we did After= and Requires= docker.service).There’s a nuance: minikube start might hang if run at boot without a tty, etc. It’s not heavily tested for headless autostart. Another approach: use cron with @reboot to run minikube start. But systemd is cleaner if it works.Be mindful that if minikube requires sudo, running as root in the service covers that.Test by rebooting the VM and seeing if kubectl get nodes works after.Now, with everything running, we do the final deployment of the Sparta app in the cloud:
That essentially is what we did: Node.js + Mongo with PV (100MB as specified) and HPA, and exposure via Nginx on port 80.
Verify that HPA on the cloud VM cluster works as well (you might need to enable metrics-server on minikube too: minikube addons enable metrics-server).
The Nginx reverse proxy is exposing it on port 80 so users can access without specifying a port.
At this point, our GitHub repository should have all manifests in a structured way (maybe in directories for each day or by component). The README should detail each step, including any issues (for example, maybe mention that on Minikube, the custom image daraymonsta/nginx-257:dreamteam needed to be pulled via docker pull because it’s on Docker Hub, or we needed to use minikube image load if it was a local image).We should also include diagrams illustrating:
Kubernetes cluster architecture (we did for Day 1).
PV/PVC architecture (Day 2).
Day 3 networking: maybe a diagram showing how external request goes to Nginx (host) -> NodePort -> app pod, etc., to clarify the flow.
Finally, ensure all logs and troubleshooting notes are included, as well as links to relevant official docs: We have cited many official docs above (Kubernetes.io, etc.) to reinforce best practices and definitions. In a real README, one would hyperlink to those docs or include references as footnotes.To conclude, check everything in the cloud environment:
Access the Node app URL (via Nginx) – ensure it talks to the database (maybe add a new item via the app if it’s that kind of test, and see it persists).
Possibly simulate a bit of load from an external source to watch HPA in effect on the VM (though limited resources might not scale much).
Ensure if the VM reboots, minikube start works (maybe test once, or mention that it needs configuration if not fully tested).
Provide instructions on how to shut down or clean up (like disabling minikube service, deleting resources if needed).
Project Documentation Deliverables

Your GitHub repository for this project should be well-structured and include all the relevant artifacts:
Kubernetes YAML Manifests: All manifests for deployments, services, config maps if any, PV/PVC, HPA, etc., organized in folders (e.g., k8s-manifests/). For clarity, you might separate them by application or by day as learned. Each YAML file should be named appropriately (e.g., nginx-deployment.yaml, sparta-app-deployment.yaml, mongo-pv.yaml, sparta-hpa.yaml, etc.).
Dockerfiles: If you containerized the Node.js Sparta app yourself or any other custom app, include the Dockerfile and instructions to build it. If images are pulled from Docker Hub (like the provided Nginx image or official Mongo), note their source. If you had to push a custom image (maybe to your Docker Hub), mention that repository.
README Documentation: A comprehensive README.md that outlines the project objectives, the step-by-step progress (likely divided by Day 1, Day 2, Day 3 sections as we have here), and how each task was achieved. This should include code snippets (applied YAMLs or commands run), and explanations for each step. It should also document any issues encountered and how you resolved them (for example, “On Day 2, encountered an error with HPA because metrics-server was missing, solved by enabling the addon” or “faced image pull error, resolved by docker login or using imagePullPolicy”). Include logs or output as proof of success (for instance, output of kubectl get hpa showing it scaled to 5 pods, or a snippet of logs showing the Node app connected to Mongo).
Kubernetes Architecture Diagrams: Include images or drawings that illustrate:
The overall Kubernetes cluster architecture (control plane vs nodes)​
DEVTRON.AI
 – likely the one from Day 1 research.
The layout of our application deployments: e.g., a diagram showing how the Node app and MongoDB are deployed in Kubernetes, how the service connects to the pods, and how the persistent volume connects to MongoDB (this could be a custom diagram focusing on our app’s architecture within k8s).
The networking flow on Day 3: a diagram showing external user -> Nginx (host) -> cluster service -> pods, etc., and how autoscaling adds more pods behind the service.
References and Links: A section in the README listing references to official documentation, blog posts, or Kubernetes API references that were helpful. Throughout the doc, when you mention a concept (like HPA, or PV), you can hyperlink to the official docs for more info​
KUBERNETES.IO
​
SPOT.IO
. We have gathered many references (Kubernetes.io docs on concepts, etc.), which should be cited. This shows you consulted reliable sources. It also helps others to follow up on deeper details. For example, link to Kubernetes’ official Persistent Volumes doc for more on storage, link to Kubernetes security best practices for the security section, etc.
Success/Failure logs: It’s good to include some snapshots of using the app (maybe a curl command output showing the Node app response), or kubectl get all outputs showing the state of the cluster at various stages. If you encountered a failure (e.g., a CrashLoopBackOff on Mongo before adding PV), mention it and how you fixed it (like "MongoDB pod restarted because no persistent storage – fixed by adding PV/PVC"). This shows the learning process and solutions.
Best Practices Followed: In narrative or bullet form, mention how you applied best practices:
Used non-root containers (if you did), or pulled official images (the Node and Mongo were official).
Configured resource requests/limits for HPA (if done).
Secured the cluster (maybe network policy could be a mention if considered).
Stored manifests in Git (IaC approach), etc.
Future improvements: (Optional) If there are things you’d do in a real production (like use Ingress controller instead of manual Nginx, use StatefulSet for Mongo with replication, set up monitoring, etc.), you can mention those as learning notes.
By delivering all the above, your project repository will allow anyone to understand what was done and even replicate the setup. They could follow the README to reproduce the cluster deployment on their own, for example.
